# -*- coding: utf-8 -*-
"""AML mini Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DkeVCJ9ySXfHV5J3FkqVx34q19PNmngl

#**AML MINI PROJECT**

#**Importing Packages**
"""

#pip install ktrain

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
import string
import ktrain
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, TFBertForSequenceClassification
from tensorflow.keras.utils import to_categorical
import tensorflow as tf
from ktrain import text
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, LSTM, Dropout, Activation, Embedding, Bidirectional,SpatialDropout1D
from keras.utils import np_utils
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix ,accuracy_score

# load and evaluate a saved model
from numpy import loadtxt
from keras.models import load_model
from google.colab import drive

"""# **Datasets**

**Reading datasets**
"""

trai_dset=pd.read_csv("/content/drive/MyDrive/Colab Notebooks/tam-sentiment-train.csv")
dev_dset=pd.read_csv("/content/drive/MyDrive/Colab Notebooks/tam-sentiment-dev.csv")
tes_dset=pd.read_csv("/content/drive/MyDrive/Colab Notebooks/ta-en_test_without_labels.csv")

"""**Dimensions of datasets**"""

print("Training dataset = {}\nDevelopment dataset = {}\nTest dataset ={}".format(trai_dset.shape,dev_dset.shape,tes_dset.shape))

"""**Label-Count in training and development dataset**"""

print("\t\tTraining dataset\n{}\n\n\t\tDevelopment dataset\n{}".format(trai_dset["category"].value_counts(),dev_dset["category"].value_counts()))

"""**Combining Training data and development data**"""

comb_dset=pd.concat([trai_dset,dev_dset])
print("Training dataset = {}\nDevelopment dataset = {}\nCombined dataset ={}".format(trai_dset.shape,dev_dset.shape,comb_dset.shape))

"""# **Preprocessing**

**Training dataset**
"""

tra_label=trai_dset.category.factorize()
oov_tok = '<OOV>'
tra_label

tr_text = trai_dset.text.values
tr_tokenizer = Tokenizer(num_words=5000,oov_token=oov_tok)
tr_tokenizer.fit_on_texts(tr_text)
tr_vocab_size = len(tr_tokenizer.word_index) + 1
tr_encoded_docs = tr_tokenizer.texts_to_sequences(tr_text)
tr_padded_sequence = pad_sequences(tr_encoded_docs, maxlen=200,padding="post",truncating='post')

"""**Development dataset**"""

dev_label=dev_dset.category.factorize()
dev_label

dev_text = dev_dset.text.values
dev_tokenizer = Tokenizer(num_words=5000,oov_token=oov_tok)
dev_tokenizer.fit_on_texts(dev_text)
dev_vocab_size = len(dev_tokenizer.word_index) + 1
dev_encoded_docs = dev_tokenizer.texts_to_sequences(dev_text)
dev_padded_sequence = pad_sequences(dev_encoded_docs, maxlen=200,padding="post",truncating='post')

"""**Combined dataset**"""

comb_label=comb_dset.category.factorize()

comb_text = comb_dset.text.values
comb_tokenizer = Tokenizer(num_words=5000,oov_token=oov_tok)
comb_tokenizer.fit_on_texts(comb_text)
comb_vocab_size = len(comb_tokenizer.word_index) + 1
comb_encoded_docs = comb_tokenizer.texts_to_sequences(comb_text)
comb_padded_sequence = pad_sequences(comb_encoded_docs, maxlen=200,padding="post",truncating='post')

print(tr_tokenizer.word_index,dev_tokenizer.word_index,sep="\n")

print(dev_encoded_docs[0])
print(dev_text[0])

"""# **Machine Learning**

**Importing Machine Learning models**
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB

"""**Training models with training data**"""

tr_RF=RandomForestClassifier(n_estimators=7)
tr_DT=DecisionTreeClassifier()
tr_LR=LogisticRegression()
tr_NB=GaussianNB()
tr_algorithms=dict(zip(["RandomForest","DecisionTree","LogisticRegression","GaussianNB"],[tr_RF,tr_DT,tr_LR,tr_NB]))
algo_names=["RandomForest","DecisionTree","LogisticRegression","GaussianNB"]
for mod in algo_names:
  tr_algorithms[mod].fit(tr_padded_sequence,trai_dset['category'])

"""**Training Model with development data**"""

dev_RF=RandomForestClassifier(n_estimators=7)
dev_DT=DecisionTreeClassifier()
dev_LR=LogisticRegression()
dev_NB=GaussianNB()
dev_algorithms=dict(zip(["RandomForest","DecisionTree","LogisticRegression","GaussianNB"],[dev_RF,dev_DT,dev_LR,dev_NB]))
algo_names=["RandomForest","DecisionTree","LogisticRegression","GaussianNB"]
for mod in algo_names:
  dev_algorithms[mod].fit(dev_padded_sequence,dev_dset['category'])

"""**Training Model with Combined data**"""

comb_RF=RandomForestClassifier(n_estimators=7)
comb_DT=DecisionTreeClassifier()
comb_LR=LogisticRegression()
comb_NB=GaussianNB()
comb_algorithms=dict(zip(["RandomForest","DecisionTree","LogisticRegression","GaussianNB"],[comb_RF,comb_DT,comb_LR,comb_NB]))
algo_names=["RandomForest","DecisionTree","LogisticRegression","GaussianNB"]
for mod in algo_names:
  comb_algorithms[mod].fit(comb_padded_sequence,comb_dset['category'])

from matplotlib import figure
from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay
def check_acc(model,inp,targ):
  ac=[]
  cm=[]
  cr=[]
  for j in algo_names:
    ac.append(accuracy_score(targ.category,model[j].predict(inp)))
    cm.append(ConfusionMatrixDisplay(confusion_matrix(targ.category,model[j].predict(inp)),display_labels=['Mixed_feelings', 'Negative', 'Positive',"unknown_state"]))
    cr.append(classification_report(targ.category,model[j].predict(inp)))
  sns.barplot(x=algo_names,y=ac)
  plt.title("---ACCURACY---")
  plt.show()
  print("\n\nCONFUSION MATRIX")
  for i in range(4):
    cm[i].plot()
    plt.title(algo_names[i])
    print(cr[i])
    plt.show()

check_acc(tr_algorithms,dev_padded_sequence,dev_dset)
check_acc(dev_algorithms,dev_padded_sequence,dev_dset)
check_acc(comb_algorithms,tr_padded_sequence,trai_dset)

"""# **Deep Learning**"""

(X_train, y_train), (X_test, y_test), preproc = text.texts_from_df(train_df=trai_dset,
                                                                   text_column = 'text',
                                                                   label_columns = 'category',
                                                                   val_df = dev_dset,
                                                                   maxlen = 200,
                                                                   preprocess_mode = 'bert')

"""**LSTM Model**"""

#pip install tensorflow
from sklearn.model_selection import RepeatedKFold
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout
from tensorflow.keras.initializers import RandomNormal
from sklearn.metrics import log_loss
def get_model(n_inputs, n_outputs):
  batch_size = 256
  hidden_units = 64
  dropout = 0.2

  model = Sequential()
  model.add(Dense(hidden_units, input_dim=n_inputs,activation='relu',
          kernel_initializer='he_uniform'))
  model.add(Dropout(dropout))
  model.add(Dense(64,activation='relu',
          kernel_initializer='he_uniform'))
  model.add(Dropout(dropout))
  model.add(Dense(n_outputs))
  model.add(Activation('sigmoid'))
  model.compile(loss='binary_crossentropy', optimizer='adam')
  return model
import tensorflow as tf
def evaluate_model(X,y):
  results_test = []
  results_train =[]
  callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5,min_delta = 0.05)
  n_inputs, n_outputs = X.shape[1], y.shape[1]
  cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
  for train_ix,test_ix in cv.split(X):
    X_train, X_test = X[train_ix], X[test_ix]
    y_train,y_test = y[train_ix],y[test_ix]
    model = get_model(n_inputs, n_outputs)
    model.fit(X_train,y_train,verbose = 0,epochs = 2,callbacks = callback)
    yhat_train = model.predict(X_train)
    yhat_test = model.predict(X_test)
    train_log_loss = log_loss(y_train, yhat_train)
    test_log_loss = log_loss(y_test,yhat_test)
    results_train.append(train_log_loss)
    results_test.append(test_log_loss)
  return results_train,results_test,model

pre=model.predict(dev_padded_sequence)
j=[np.argmax(i) for i in pre]
pred=pd.DataFrame(j)
accuracy_score(dev_label[0],pred)

"""**Bert Model**"""

(X_train, y_train), (X_test, y_test), preproc = text.texts_from_df(train_df=trai_dset,
                                                                   text_column = 'text',
                                                                   label_columns = 'category',
                                                                   val_df = dev_dset,
                                                                   maxlen = 200,
                                                                   preprocess_mode = 'bert')

model = text.text_classifier(name = 'bert',
                             train_data = (X_train, y_train),
                             preproc = preproc)

learner = ktrain.get_learner(model=model, train_data=(X_train, y_train),
                   val_data = (X_test, y_test),
                   batch_size = 64)

"""**Finding best learnerRate**"""

learner.fit_onecycle(lr = 2e-05, epochs = 2)-
predictor = ktrain.get_predictor(learner.model, preproc)

pre=model.predict(dev_padded_sequence)
j=[np.argmax(i) for i in pre]
pred=pd.DataFrame(j)
accuracy_score(dev_label[0],pred)



import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from transformers import BertTokenizer, TFBertForSequenceClassification
from tensorflow.keras.preprocessing.sequence import pad_sequences
trai_dset=pd.read_csv("/content/drive/MyDrive/Colab Notebooks/tam-sentiment-train.csv")
dev_dset=pd.read_csv("/content/drive/MyDrive/Colab Notebooks/tam-sentiment-dev.csv")
tes_dset=pd.read_csv("/content/drive/MyDrive/Colab Notebooks/ta-en_test_without_labels.csv")
# Load the dataset containing mixed Tamil-English text and sentiment labels
dataset =trai_dset  # Replace with your dataset file

# Preprocess the data
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')

# Tokenize and encode the mixed text
encoded_data = tokenizer.batch_encode_plus(
    dev_dset['text'].values,
    add_special_tokens=True,
    return_attention_mask=True,
    padding='max_length',
    max_length=128,
    truncation=True,
    return_tensors='tf'
)

input_ids = np.array(encoded_data['input_ids'])
attention_masks = np.array(encoded_data['attention_mask'])
labels = np.array(dev_dset['category'])

# Perform label encoding on the sentiment labels
label_encoder = LabelEncoder()
labels = label_encoder.fit_transform(labels)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(input_ids, labels, test_size=0.2, random_state=42)
train_masks, test_masks, _, _ = train_test_split(attention_masks, input_ids, test_size=0.2, random_state=42)

# Load the pre-trained BERT model for sequence classification
model = TFBertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=len(label_encoder.classes_))

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(
    [X_train, train_masks],
    y_train,
    epochs=2,
    batch_size=8,
    validation_data=([X_test, test_masks], y_test)
)

# Evaluate the model
loss, accuracy = model.evaluate([X_test, test_masks], y_test)
print("Loss:", loss)
print("Accuracy:", accuracy)

labels[0],dataset[0,::]

test_dset=pd.DataFrame({'Text':tes_dset.values[:,0]})

inp=tokenizer.batch_encode_plus(
    test_dset['Text'].values,
    add_special_tokens=True,
    return_attention_mask=True,
    padding='max_length',
    max_length=200,
    truncation=True,
    return_tensors='tf'
)

inf = np.array(inp['input_ids'])
inf.shape

pre=model.predict(inf)

de=label_encoder.classes_.tolist()
j=[de[np.argmax(i)] for i in pre[0]]
pred=pd.DataFrame({"Predicted_Class":j})
Run=pd.concat([test_dset,pred],axis=1)
Run.Predicted_Class.unique()

import pandas as pd
csv_file_path = '/content/drive/MyDrive/Colab Notebooks/Run.csv'

Run.to_csv(csv_file_path, index=False)

print("CSV file created successfully!")

j=tr_DT.predict(inf)
pred=pd.DataFrame({"Predicted_Class":j})
Run=pd.concat([test_dset,pred],axis=1)
Run.Predicted_Class.value_counts()

import numpy as array
a=np.array[promoth]
print("prompoth")

